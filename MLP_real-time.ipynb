{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "494ffc15-90b1-4d0d-ab13-864da980267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b1cad5df-a939-44bf-b433-6ce842ed4a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x219f54f76a0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "import torchvision\n",
    "\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from skimage import io, transform\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.ion()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5964ab95-d55b-4566-abe8-1e83677718ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "410b8ad2-4131-45ab-974d-1f5e778af817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15.2+cu118\n"
     ]
    }
   ],
   "source": [
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2bac5a4f-c53f-4f4f-8934-d1679b7394e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b53b0db-2924-460a-84e9-7e3bc90b1936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best.pt\n",
      "CNN_0.962963.pt\n",
      "MLP_0.761364.pt\n",
      "MLP_0.851064.pt\n",
      "MLP_0.872340.pt\n",
      "MLP_0.888889new_model3.pt\n",
      "MLP_0.893617.pt\n",
      "MLP_0.914894.pt\n",
      "MLP_0.919753.pt\n",
      "MLP_0.919753new_model2.pt\n",
      "MLP_0.932099new_model3.pt\n",
      "MLP_0.936170.pt\n",
      "MLP_0.936170_best.pt\n",
      "MLP_0.936170_old.pt\n",
      "MLP_0.938272new_model3.pt\n"
     ]
    }
   ],
   "source": [
    "for model_dir in os.listdir(\"./\"):\n",
    "    if model_dir.split(\".\")[-1] != \"pt\":\n",
    "        continue\n",
    "    print(model_dir)   \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3ee939d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f224af",
   "metadata": {},
   "source": [
    "## first model of ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a3b6eb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "MLP_Audio_Model2(\n",
      "  (linear1): Linear(in_features=2790, out_features=256, bias=True)\n",
      "  (bachNoem1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): ReLU()\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (bachNoem2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): ReLU()\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (linear5): Linear(in_features=64, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP_Audio_Model2(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP_Audio_Model2, self).__init__()\n",
    "\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(2790, 256)\n",
    "        self.bachNoem1 = nn.BatchNorm1d(256)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.linear2 = torch.nn.Linear(256, 64)\n",
    "        self.bachNoem2 = nn.BatchNorm1d(64)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "\n",
    "#         self.linear3 = torch.nn.Linear(128, 32)\n",
    "#         self.bachNoem3 = torch.nn.BatchNorm1d(32)\n",
    "#         self.dropout3 = nn.Dropout(p=0.1)\n",
    "#         self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "        # self.linear4 = torch.nn.Linear(128, 32)\n",
    "        # self.bachNoem4 = torch.nn.BatchNorm1d(32)\n",
    "        # self.dropout4 = nn.Dropout(p=0.3)\n",
    "        # self.activation4 = torch.nn.ReLU()\n",
    "        \n",
    "#         self.linear6 = torch.nn.Linear(64, 32)\n",
    "#         self.bachNoem6 = torch.nn.BatchNorm1d(32)\n",
    "#         self.dropout6 = nn.Dropout(p=0.1)\n",
    "#         self.activation6 = torch.nn.ReLU()\n",
    "        \n",
    "#         self.linear7 = torch.nn.Linear(32, 16)\n",
    "#         self.bachNoem7 = torch.nn.BatchNorm1d(16)\n",
    "#         self.dropout7 = nn.Dropout(p=0.1)\n",
    "#         self.activation7 = torch.nn.ReLU()\n",
    "        \n",
    "#         self.linear8 = torch.nn.Linear(32, 16)\n",
    "#         self.bachNoem8 = torch.nn.BatchNorm1d(16)\n",
    "#         self.dropout8 = nn.Dropout(p=0.2)\n",
    "#         self.activation8 = torch.nn.ReLU()\n",
    "        \n",
    "#         self.linear9 = torch.nn.Linear(32, 16)\n",
    "#         self.bachNoem9 = torch.nn.BatchNorm1d(16)\n",
    "#         self.dropout9 = nn.Dropout(p=0.2)\n",
    "#         self.activation9 = torch.nn.ReLU()\n",
    "\n",
    "        self.linear5 = torch.nn.Linear(64, 4)\n",
    "\n",
    "        self.softmax = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.bachNoem1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.bachNoem2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "#         x = self.linear3(x)\n",
    "#         x = self.bachNoem3(x)\n",
    "#         x = self.dropout3(x)\n",
    "#         x = self.activation3(x)\n",
    "\n",
    "        # x = self.linear4(x)\n",
    "        # x = self.bachNoem4(x)\n",
    "        # x = self.dropout4(x)\n",
    "        # x = self.activation4(x)        \n",
    "\n",
    "#         x = self.linear6(x)\n",
    "#         x = self.bachNoem6(x)\n",
    "#         # x = self.dropout6(x)\n",
    "#         x = self.activation6(x)      \n",
    "        \n",
    "#         x = self.linear7(x)\n",
    "#         x = self.bachNoem7(x)\n",
    "#         x = self.dropout7(x)\n",
    "#         x = self.activation7(x)\n",
    "        \n",
    "        x = self.linear5(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "MLP_model1= MLP_Audio_Model2()\n",
    "\n",
    "print('The model:')\n",
    "print(MLP_model1)\n",
    "\n",
    "# print('\\n\\nModel params:')\n",
    "# for param in MLP_model.parameters():\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c25eaf29-81db-4ee6-9cfa-dff233993fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "model_path = current_dir + '\\\\' + \"MLP_0.919753new_model2.pt\"\n",
    "\n",
    "test_model_1 = MLP_Audio_Model2()\n",
    "\n",
    "test_model_1 = torch.load(model_path)\n",
    "\n",
    "\n",
    "# test_model = torch.load(model_path)\n",
    "\n",
    "test_model_1.eval()\n",
    "test_model_1 = test_model_1.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f637f98a",
   "metadata": {},
   "source": [
    "## second model of ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4cb8e881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "MLP_Audio_Model3(\n",
      "  (linear1): Linear(in_features=2790, out_features=512, bias=True)\n",
      "  (bachNoem1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): ReLU()\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (linear3): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (bachNoem3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (activation3): ReLU()\n",
      "  (linear6): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (bachNoem6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout6): Dropout(p=0.5, inplace=False)\n",
      "  (activation6): ReLU()\n",
      "  (linear8): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (bachNoem8): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout8): Dropout(p=0.5, inplace=False)\n",
      "  (activation8): ReLU()\n",
      "  (linear5): Linear(in_features=16, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP_Audio_Model3(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP_Audio_Model3, self).__init__()\n",
    "\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(2790, 512)\n",
    "        self.bachNoem1 = nn.BatchNorm1d(512)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "\n",
    "        # self.linear2 = torch.nn.Linear(1024, 512)\n",
    "        # self.bachNoem2 = nn.BatchNorm1d(512)\n",
    "        # self.activation2 = torch.nn.ReLU()\n",
    "        # self.dropout2 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.linear3 = torch.nn.Linear(512, 128)\n",
    "        self.bachNoem3 = torch.nn.BatchNorm1d(128)\n",
    "        self.dropout3 = nn.Dropout(p=0.5)\n",
    "        self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "#         self.linear4 = torch.nn.Linear(256, 128)\n",
    "#         self.bachNoem4 = torch.nn.BatchNorm1d(128)\n",
    "#         self.dropout4 = nn.Dropout(p=0.3)\n",
    "#         self.activation4 = torch.nn.ReLU()\n",
    "        \n",
    "        self.linear6 = torch.nn.Linear(128, 64)\n",
    "        self.bachNoem6 = torch.nn.BatchNorm1d(64)\n",
    "        self.dropout6 = nn.Dropout(p=0.5)\n",
    "        self.activation6 = torch.nn.ReLU()\n",
    "        \n",
    "        # self.linear7 = torch.nn.Linear(64, 32)\n",
    "        # self.bachNoem7 = torch.nn.BatchNorm1d(32)\n",
    "        # self.dropout7 = nn.Dropout(p=0.5)\n",
    "        # self.activation7 = torch.nn.ReLU()\n",
    "        \n",
    "        self.linear8 = torch.nn.Linear(64, 16)\n",
    "        self.bachNoem8 = torch.nn.BatchNorm1d(16)\n",
    "        self.dropout8 = nn.Dropout(p=0.5)\n",
    "        self.activation8 = torch.nn.ReLU()\n",
    "        \n",
    "#         self.linear9 = torch.nn.Linear(32, 16)\n",
    "#         self.bachNoem9 = torch.nn.BatchNorm1d(16)\n",
    "#         self.dropout9 = nn.Dropout(p=0.2)\n",
    "#         self.activation9 = torch.nn.ReLU()\n",
    "\n",
    "        self.linear5 = torch.nn.Linear(16, 4)\n",
    "\n",
    "        self.softmax = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.bachNoem1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # x = self.linear2(x)\n",
    "        # x = self.bachNoem2(x)\n",
    "        # x = self.activation2(x)\n",
    "        # x = self.dropout2(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "        x = self.bachNoem3(x)\n",
    "        x = self.activation3(x)\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        # x = self.linear4(x)\n",
    "        # x = self.bachNoem4(x)\n",
    "        # x = self.activation4(x)\n",
    "        # x = self.dropout4(x)\n",
    "\n",
    "        x = self.linear6(x)\n",
    "        x = self.bachNoem6(x)\n",
    "        x = self.activation6(x)\n",
    "        x = self.dropout6(x)\n",
    "        \n",
    "        # x = self.linear7(x)\n",
    "        # x = self.bachNoem7(x)\n",
    "        # x = self.activation7(x)\n",
    "        # x = self.dropout7(x)\n",
    "        \n",
    "        x = self.linear8(x)\n",
    "        x = self.bachNoem8(x)\n",
    "        x = self.activation8(x)\n",
    "        x = self.dropout8(x)   \n",
    "        \n",
    "        \n",
    "        x = self.linear5(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "MLP_model = MLP_Audio_Model3()\n",
    "\n",
    "print('The model:')\n",
    "print(MLP_model)\n",
    "\n",
    "# print('\\n\\nModel params:')\n",
    "# for param in MLP_model.parameters():\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "640fa28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_Audio_Model3(\n",
      "  (linear1): Linear(in_features=2790, out_features=512, bias=True)\n",
      "  (bachNoem1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): ReLU()\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (linear3): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (bachNoem3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout3): Dropout(p=0.5, inplace=False)\n",
      "  (activation3): ReLU()\n",
      "  (linear6): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (bachNoem6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout6): Dropout(p=0.5, inplace=False)\n",
      "  (activation6): ReLU()\n",
      "  (linear8): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (bachNoem8): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout8): Dropout(p=0.5, inplace=False)\n",
      "  (activation8): ReLU()\n",
      "  (linear5): Linear(in_features=16, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "model_path = current_dir + '\\\\' + \"MLP_0.888889new_model3.pt\"\n",
    "\n",
    "test_model_2 = MLP_Audio_Model3()\n",
    "\n",
    "test_model_2 = torch.load(model_path)\n",
    "\n",
    "\n",
    "test_model_2.eval()\n",
    "test_model_2 = test_model_2.to(device)\n",
    "print(test_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6144149",
   "metadata": {},
   "source": [
    "## third model of ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f0c6e9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "MLP_Audio_Model(\n",
      "  (linear1): Linear(in_features=2790, out_features=512, bias=True)\n",
      "  (bachNoem1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): ReLU()\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (bachNoem2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): ReLU()\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (linear4): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (bachNoem4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout4): Dropout(p=0.3, inplace=False)\n",
      "  (activation4): ReLU()\n",
      "  (linear5): Linear(in_features=32, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MLP_Audio_Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP_Audio_Model, self).__init__()\n",
    "\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(2790, 512)\n",
    "        self.bachNoem1 = nn.BatchNorm1d(512)\n",
    "        self.activation1 = torch.nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "\n",
    "        self.linear2 = torch.nn.Linear(512, 128)\n",
    "        self.bachNoem2 = nn.BatchNorm1d(128)\n",
    "        self.activation2 = torch.nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "\n",
    "#         self.linear3 = torch.nn.Linear(128, 32)\n",
    "#         self.bachNoem3 = torch.nn.BatchNorm1d(32)\n",
    "#         self.dropout3 = nn.Dropout(p=0.1)\n",
    "#         self.activation3 = torch.nn.ReLU()\n",
    "\n",
    "        self.linear4 = torch.nn.Linear(128, 32)\n",
    "        self.bachNoem4 = torch.nn.BatchNorm1d(32)\n",
    "        self.dropout4 = nn.Dropout(p=0.3)\n",
    "        self.activation4 = torch.nn.ReLU()\n",
    "        \n",
    "#         self.linear6 = torch.nn.Linear(64, 32)\n",
    "#         self.bachNoem6 = torch.nn.BatchNorm1d(32)\n",
    "#         self.dropout6 = nn.Dropout(p=0.1)\n",
    "#         self.activation6 = torch.nn.ReLU()\n",
    "        \n",
    "#         self.linear7 = torch.nn.Linear(32, 16)\n",
    "#         self.bachNoem7 = torch.nn.BatchNorm1d(16)\n",
    "#         self.dropout7 = nn.Dropout(p=0.1)\n",
    "#         self.activation7 = torch.nn.ReLU()\n",
    "        \n",
    "#         self.linear8 = torch.nn.Linear(32, 16)\n",
    "#         self.bachNoem8 = torch.nn.BatchNorm1d(16)\n",
    "#         self.dropout8 = nn.Dropout(p=0.2)\n",
    "#         self.activation8 = torch.nn.ReLU()\n",
    "        \n",
    "#         self.linear9 = torch.nn.Linear(32, 16)\n",
    "#         self.bachNoem9 = torch.nn.BatchNorm1d(16)\n",
    "#         self.dropout9 = nn.Dropout(p=0.2)\n",
    "#         self.activation9 = torch.nn.ReLU()\n",
    "\n",
    "        self.linear5 = torch.nn.Linear(32, 4)\n",
    "\n",
    "        self.softmax = torch.nn.Softmax(dim = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.bachNoem1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = self.bachNoem2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "#         x = self.linear3(x)\n",
    "#         x = self.bachNoem3(x)\n",
    "#         x = self.dropout3(x)\n",
    "#         x = self.activation3(x)\n",
    "\n",
    "        x = self.linear4(x)\n",
    "        x = self.bachNoem4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.activation4(x)        \n",
    "\n",
    "#         x = self.linear6(x)\n",
    "#         x = self.bachNoem6(x)\n",
    "#         # x = self.dropout6(x)\n",
    "#         x = self.activation6(x)      \n",
    "        \n",
    "#         x = self.linear7(x)\n",
    "#         x = self.bachNoem7(x)\n",
    "#         x = self.dropout7(x)\n",
    "#         x = self.activation7(x)\n",
    "        \n",
    "        x = self.linear5(x)\n",
    "\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "MLP_model = MLP_Audio_Model()\n",
    "\n",
    "print('The model:')\n",
    "print(MLP_model)\n",
    "\n",
    "# print('\\n\\nModel params:')\n",
    "# for param in MLP_model.parameters():\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4f3beadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_Audio_Model(\n",
      "  (linear1): Linear(in_features=2790, out_features=512, bias=True)\n",
      "  (bachNoem1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation1): ReLU()\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "  (bachNoem2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation2): ReLU()\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (linear4): Linear(in_features=128, out_features=32, bias=True)\n",
      "  (bachNoem4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (dropout4): Dropout(p=0.3, inplace=False)\n",
      "  (activation4): ReLU()\n",
      "  (linear5): Linear(in_features=32, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "model_path = current_dir + '\\\\' + \"best.pt\"\n",
    "\n",
    "test_model_3 = MLP_Audio_Model()\n",
    "\n",
    "test_model_3 = torch.load(model_path)\n",
    "\n",
    "\n",
    "test_model_3.eval()\n",
    "test_model_3 = test_model_3.to(device)\n",
    "print(test_model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9494c572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import math\n",
    "from scipy import signal\n",
    "import soundfile as sf\n",
    "import gradio as gr\n",
    "import sounddevice as sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aad6eb",
   "metadata": {},
   "source": [
    "## proccess function for inputs(ensemble learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bebdefaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "\n",
    "def proccess(frame):\n",
    "    \n",
    "        dir_dict = {0: \"Down\", 1: \"Left\", 2: \"Right\", 3: \"Up\"}\n",
    "        \n",
    "        tensor = frame\n",
    "\n",
    "        print(tensor.size())\n",
    "        \n",
    "#         target_size = 48000\n",
    "#         tensor = torch.nn.functional.pad(tensor, (0, target_size - tensor.size(1), 0, 0), 'constant', 0)\n",
    "        \n",
    "        print(tensor.size())\n",
    "\n",
    "\n",
    "        live_transform = T.MFCC(\n",
    "            sample_rate=SAMPLE_RATE,\n",
    "            n_mfcc=31,\n",
    "            melkwargs={\"n_fft\": 2048  , \"hop_length\": 512 , \"n_mels\": 128, \"center\": False}\n",
    "        )\n",
    "\n",
    "        tensor = live_transform(tensor)\n",
    "        \n",
    "        tensor = nn.functional.normalize(tensor, dim=-1)\n",
    "\n",
    "        print(tensor.size())\n",
    "\n",
    "        tensor = torch.flatten(tensor)\n",
    "\n",
    "        tensor = torch.reshape(tensor, (1, 2790))\n",
    "        \n",
    "        tensor = tensor.to(device)\n",
    "\n",
    "        output_1 = test_model_1(tensor)\n",
    "        selected_1 = torch.argmax(output_1.to(\"cpu\"), dim=1)\n",
    "        output_2 = test_model_2(tensor)\n",
    "        selected_2 = torch.argmax(output_2.to(\"cpu\"), dim=1)\n",
    "        output_3 = test_model_3(tensor)\n",
    "        selected_3 = torch.argmax(output_3.to(\"cpu\"), dim=1)\n",
    "        \n",
    "        output = (output_1*3 + output_2*6 + output_3)/10\n",
    "        \n",
    "        preds = output\n",
    "        \n",
    "        print(output_1 , \"\\n\", output_2, \"\\n\", output_3, \"\\n\",  preds)\n",
    "\n",
    "        predicted_value = torch.argmax(preds.to(\"cpu\"), dim=1)\n",
    "        \n",
    "        if(selected_1[0] == 3 or selected_2 == 3):\n",
    "            predicted_value = torch.tensor([3], dtype=torch.float16)\n",
    "            \n",
    "        if output_1[0][selected_1[0]] > 0.9:\n",
    "            predicted_value = selected_1\n",
    "        elif output_2[0][selected_2[0]] > 0.9:\n",
    "            predicted_value = selected_2\n",
    "        elif output_2[0][selected_3[0]] > 0.9:\n",
    "            predicted_value = selected_3\n",
    "        \n",
    "\n",
    "        # Return the output as a numpy array\n",
    "        return dir_dict[predicted_value.detach().numpy().item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3317a9",
   "metadata": {},
   "source": [
    "## real-time block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9654f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n",
      "duration: 3.0705764293670654\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.7939, 0.1124, 0.0773, 0.0165]], device='cuda:0') \n",
      " tensor([[0.6528, 0.0818, 0.1728, 0.0926]], device='cuda:0') \n",
      " tensor([[0.4693, 0.0102, 0.5128, 0.0077]], device='cuda:0') \n",
      " tensor([[0.6768, 0.0838, 0.1781, 0.0613]], device='cuda:0')\n",
      "Predicted label: [0]\n",
      "Recording...\n",
      "duration: 3.0708789825439453\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.0395, 0.8962, 0.0612, 0.0031]], device='cuda:0') \n",
      " tensor([[0.0503, 0.5540, 0.3752, 0.0205]], device='cuda:0') \n",
      " tensor([[0.0193, 0.0065, 0.9719, 0.0024]], device='cuda:0') \n",
      " tensor([[0.0440, 0.6019, 0.3407, 0.0135]], device='cuda:0')\n",
      "Predicted label: [1]\n",
      "Recording...\n",
      "duration: 3.070493459701538\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.0761, 0.2090, 0.6823, 0.0326]], device='cuda:0') \n",
      " tensor([[0.0455, 0.3692, 0.5624, 0.0228]], device='cuda:0') \n",
      " tensor([[0.0152, 0.0257, 0.9497, 0.0093]], device='cuda:0') \n",
      " tensor([[0.0517, 0.2868, 0.6371, 0.0244]], device='cuda:0')\n",
      "Predicted label: [2]\n",
      "Recording...\n",
      "duration: 3.070827007293701\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.6240, 0.3373, 0.0315, 0.0072]], device='cuda:0') \n",
      " tensor([[0.1691, 0.5472, 0.2358, 0.0478]], device='cuda:0') \n",
      " tensor([[0.0309, 0.0180, 0.9431, 0.0079]], device='cuda:0') \n",
      " tensor([[0.2917, 0.4313, 0.2453, 0.0317]], device='cuda:0')\n",
      "Predicted label: [1]\n",
      "Recording...\n",
      "duration: 3.070296287536621\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.0235, 0.6725, 0.0961, 0.2079]], device='cuda:0') \n",
      " tensor([[0.1043, 0.5673, 0.2876, 0.0408]], device='cuda:0') \n",
      " tensor([[0.0651, 0.0692, 0.8327, 0.0331]], device='cuda:0') \n",
      " tensor([[0.0761, 0.5490, 0.2846, 0.0902]], device='cuda:0')\n",
      "Predicted label: [1]\n",
      "Recording...\n",
      "duration: 3.0697364807128906\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.8046, 0.1269, 0.0522, 0.0164]], device='cuda:0') \n",
      " tensor([[0.1858, 0.2260, 0.5233, 0.0649]], device='cuda:0') \n",
      " tensor([[0.5677, 0.0270, 0.3646, 0.0406]], device='cuda:0') \n",
      " tensor([[0.4096, 0.1764, 0.3661, 0.0479]], device='cuda:0')\n",
      "Predicted label: [0]\n",
      "Recording...\n",
      "duration: 3.070852279663086\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.0165, 0.9492, 0.0205, 0.0138]], device='cuda:0') \n",
      " tensor([[0.1080, 0.5370, 0.3148, 0.0401]], device='cuda:0') \n",
      " tensor([[0.1329, 0.1773, 0.6133, 0.0766]], device='cuda:0') \n",
      " tensor([[0.0830, 0.6247, 0.2564, 0.0359]], device='cuda:0')\n",
      "Predicted label: [1]\n",
      "Recording...\n",
      "duration: 3.0704095363616943\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.3368, 0.1780, 0.3290, 0.1561]], device='cuda:0') \n",
      " tensor([[0.3685, 0.1316, 0.4131, 0.0867]], device='cuda:0') \n",
      " tensor([[0.1725, 0.0149, 0.7915, 0.0212]], device='cuda:0') \n",
      " tensor([[0.3394, 0.1339, 0.4257, 0.1010]], device='cuda:0')\n",
      "Predicted label: [2]\n",
      "Recording...\n",
      "duration: 3.0708870887756348\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.2708, 0.4131, 0.2716, 0.0446]], device='cuda:0') \n",
      " tensor([[0.2151, 0.2741, 0.4439, 0.0669]], device='cuda:0') \n",
      " tensor([[0.0344, 0.0133, 0.9369, 0.0154]], device='cuda:0') \n",
      " tensor([[0.2137, 0.2897, 0.4415, 0.0551]], device='cuda:0')\n",
      "Predicted label: [2]\n",
      "Recording...\n",
      "duration: 3.0708303451538086\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.6119, 0.3573, 0.0140, 0.0168]], device='cuda:0') \n",
      " tensor([[0.4050, 0.3558, 0.1525, 0.0867]], device='cuda:0') \n",
      " tensor([[0.4638, 0.1853, 0.3167, 0.0342]], device='cuda:0') \n",
      " tensor([[0.4730, 0.3392, 0.1273, 0.0605]], device='cuda:0')\n",
      "Predicted label: [0]\n",
      "Recording...\n",
      "duration: 3.0694267749786377\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.6733, 0.1333, 0.1619, 0.0314]], device='cuda:0') \n",
      " tensor([[0.3815, 0.3093, 0.2245, 0.0847]], device='cuda:0') \n",
      " tensor([[0.1954, 0.0477, 0.7168, 0.0401]], device='cuda:0') \n",
      " tensor([[0.4504, 0.2303, 0.2550, 0.0643]], device='cuda:0')\n",
      "Predicted label: [0]\n",
      "Recording...\n",
      "duration: 3.070161819458008\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.5648, 0.3171, 0.0297, 0.0884]], device='cuda:0') \n",
      " tensor([[0.3299, 0.2209, 0.3214, 0.1278]], device='cuda:0') \n",
      " tensor([[0.2822, 0.0999, 0.5700, 0.0479]], device='cuda:0') \n",
      " tensor([[0.3956, 0.2377, 0.2587, 0.1080]], device='cuda:0')\n",
      "Predicted label: [0]\n",
      "Recording...\n",
      "duration: 3.070190906524658\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.9724, 0.0078, 0.0040, 0.0158]], device='cuda:0') \n",
      " tensor([[0.6576, 0.0813, 0.1155, 0.1456]], device='cuda:0') \n",
      " tensor([[0.3804, 0.0189, 0.5808, 0.0199]], device='cuda:0') \n",
      " tensor([[0.7243, 0.0530, 0.1286, 0.0941]], device='cuda:0')\n",
      "Predicted label: [0]\n",
      "Recording...\n",
      "duration: 3.0710434913635254\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.0840, 0.8328, 0.0796, 0.0036]], device='cuda:0') \n",
      " tensor([[0.2065, 0.3319, 0.4082, 0.0534]], device='cuda:0') \n",
      " tensor([[0.0766, 0.0045, 0.9165, 0.0024]], device='cuda:0') \n",
      " tensor([[0.1568, 0.4494, 0.3605, 0.0333]], device='cuda:0')\n",
      "Predicted label: [1]\n",
      "Recording...\n",
      "duration: 3.0705416202545166\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.9611, 0.0025, 0.0354, 0.0011]], device='cuda:0') \n",
      " tensor([[0.2414, 0.0933, 0.6043, 0.0610]], device='cuda:0') \n",
      " tensor([[0.3349, 0.0027, 0.6569, 0.0055]], device='cuda:0') \n",
      " tensor([[0.4667, 0.0570, 0.4389, 0.0375]], device='cuda:0')\n",
      "Predicted label: [0]\n",
      "Recording...\n",
      "duration: 3.0699164867401123\n",
      "Processing...\n",
      "(48000,)\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 48000])\n",
      "torch.Size([1, 31, 90])\n",
      "tensor([[0.0702, 0.0148, 0.3698, 0.5452]], device='cuda:0') \n",
      " tensor([[0.0907, 0.1036, 0.7477, 0.0579]], device='cuda:0') \n",
      " tensor([[0.0260, 0.0067, 0.9506, 0.0167]], device='cuda:0') \n",
      " tensor([[0.0781, 0.0673, 0.6546, 0.2000]], device='cuda:0')\n",
      "Predicted label: [3.]\n",
      "Recording...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m recorded_frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mint\u001b[39m(input_sample_rate \u001b[38;5;241m/\u001b[39m chunk_size \u001b[38;5;241m*\u001b[39m output_duration) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 28\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m     recorded_frames\u001b[38;5;241m.\u001b[39mappend(data)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Stop the stream\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot input stream\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_format = pyaudio.paInt16\n",
    "input_channels = 1\n",
    "input_sample_rate = 16000  # Changed to 16000 samples per second\n",
    "chunk_size = 2048\n",
    "output_format = 'WAV'\n",
    "output_channels = 1\n",
    "output_sample_rate = 16000\n",
    "output_duration = 3  # seconds\n",
    "num_output_frames = output_sample_rate * output_duration\n",
    "\n",
    "# Create a PyAudio object\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "while True:\n",
    "    time.sleep(3)\n",
    "    print('Recording...')\n",
    "\n",
    "    # Open the microphone stream\n",
    "    stream = audio.open(format=input_format, channels=input_channels, rate=input_sample_rate, input=True, frames_per_buffer=chunk_size)\n",
    "\n",
    "    # Start the stream\n",
    "    since = time.time()\n",
    "    stream.start_stream()\n",
    "\n",
    "    # Record 3 seconds of audio\n",
    "    recorded_frames = []\n",
    "    for i in range(int(input_sample_rate / chunk_size * output_duration) + 1):\n",
    "        data = stream.read(chunk_size)\n",
    "        recorded_frames.append(data)\n",
    "\n",
    "    # Stop the stream\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f\"duration: {time_elapsed}\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "\n",
    "    print('Processing...')\n",
    "\n",
    "    # Convert the recorded audio to a NumPy array\n",
    "    recorded_data = np.frombuffer(b''.join(recorded_frames), dtype=np.int16)\n",
    "    recorded_data = recorded_data[:48000]\n",
    "    sd.play(recorded_data, 16000)\n",
    "    recorded_data = recorded_data.astype(np.float16)\n",
    "    print(recorded_data.shape)\n",
    "    \n",
    "    # Reshape the audio to (1, 48000) size\n",
    "    audio_tensor = torch.from_numpy(recorded_data).reshape(1, -1)\n",
    "    print(audio_tensor.size())\n",
    "    # Process the audio with the pre-trained model\n",
    "    with torch.no_grad():\n",
    "        output = proccess(audio_tensor)\n",
    "\n",
    "    # Print the predicted label\n",
    "    print(f'Predicted label: {output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e144a9af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3072fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
